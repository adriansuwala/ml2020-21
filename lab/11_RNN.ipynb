{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P95c6hK3hAQq"
      },
      "source": [
        "# Rekurencyjne Sieci Neuronowe (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laVdd5g5hAQu"
      },
      "source": [
        "### Importy i Utilsy  (odpalić i schować )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0D3yk7lhAQu"
      },
      "source": [
        "# imports \n",
        "import torch\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "all_letters = string.ascii_letters\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, targets):\n",
        "        \n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __getitem__(self, ind):\n",
        "        \n",
        "        return self.data[ind], self.targets[ind]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    \n",
        "def unicode_to__ascii(s: str) -> str:\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
        "                                                                 and c in all_letters)\n",
        "                   \n",
        "\n",
        "def read_lines(filename: str) -> List[str]:\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicode_to__ascii(line) for line in lines]\n",
        "\n",
        "\n",
        "def letter_to_index(letter: str) -> int:\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "\n",
        "def line_to_tensor(line: str) -> torch.Tensor:\n",
        "    tensor = torch.zeros(len(line), n_letters)\n",
        "    for i, letter in enumerate(line):\n",
        "        tensor[i][letter_to_index(letter)] = 1\n",
        "    return tensor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcSQvaMPhAQv"
      },
      "source": [
        "## Dane sekwencyjne\n",
        "\n",
        "Modele, którymi zajmowaliśmy się wcześniej zakładały konkretny kształt danych. Dla przykładu klasyczna sieć neuronowa fully-connected dla MNISTa zakładała, że na wejściu dostanie wektory rozmiaru 784 - dla wektorów o innej wymiarowości i innych obiektów model zwyczajnie nie będzie działać.\n",
        "\n",
        "Takie założenie bywa szczególnie niewygodne przy pracy z niektórymi typami danych, takimi jak:\n",
        "* językiem naturalny (słowa czy zdania mają zadanej z góry liczby znaków)\n",
        "* szeregi czasowe (dane giełdowe ciągną się właściwie w nieskończoność) \n",
        "* dźwięk (nagrania mogą być krótsze lub dłuższe).\n",
        "\n",
        "Do rozwiązania tego problemu służą rekuencyjne sieci neuronowe (*recurrent neural networks, RNNs*), które zapamiętują swój stan z poprzedniej iteracji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH3chO87hAQv"
      },
      "source": [
        "### Ładowanie danych\n",
        "Poniższe dwie komórki ściągają dataset nazwisk z 18 różnych narodowości. Każda litera w danym nazwisku jest zamieniana na jej indeks z alfabetu w postaci kodowania \"one-hot\". Inaczej mówiąc, każde nazwisko jest binarną macierzą rozmiaru `len(name)` $\\times$ `n_letters`. \n",
        "\n",
        "Dodatkowo, ponieważ ten dataset jest mocno niezbalansowany, użyjemy specjalnego samplera do losowania przykładów treningowych, tak aby do uczenia sieć widziała tyle samo przykładów z każdej klasy.\n",
        "\n",
        "Ponieważ nazwiska mogą mieć różne długości będziemy rozważać `batch_size = 1` w tym notebooku (choć implementacje modeli powinny działać dla dowolnych wartości `batch_size`!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maOHB6NZiRgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55d4740-10b0-468a-fde4-f39ec38a9cd2"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-16 14:28:57--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 99.84.222.65, 99.84.222.94, 99.84.222.10, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|99.84.222.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-01-16 14:28:57 (72.8 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRGjkPZ2hAQv"
      },
      "source": [
        "# NOTE: you can change the seed or remove it completely if you like\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "data_dir = 'data/names'\n",
        "\n",
        "data = []\n",
        "targets = [] \n",
        "label_to_idx = {}\n",
        "\n",
        "# read each natonality file and process data \n",
        "for label, file_name in enumerate(os.listdir(data_dir)):\n",
        "    \n",
        "    label_to_idx[label] = file_name.split('.')[0].lower()\n",
        "    \n",
        "    names = read_lines(os.path.join(data_dir, file_name))\n",
        "    data += [line_to_tensor(name) for name in names]\n",
        "    targets += len(names) * [label]\n",
        "\n",
        "# split into train and test indices\n",
        "test_frac = 0.1\n",
        "n_test = int(test_frac * len(targets))\n",
        "test_ind = np.random.choice(len(targets), size=n_test, replace=False)\n",
        "train_ind = np.setdiff1d(np.arange(len(targets)), test_ind)\n",
        "\n",
        "targets = torch.tensor(targets)\n",
        "train_targets = targets[train_ind]\n",
        "\n",
        "# calculate weights for BalancedSampler\n",
        "uni, counts = np.unique(train_targets, return_counts=True)\n",
        "weight_per_class = len(targets) / counts\n",
        "weight = [weight_per_class[c] for c in train_targets]\n",
        "# preapre the sampler\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weight, num_samples=len(weight)) \n",
        "\n",
        "train_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in train_ind], targets=train_targets)\n",
        "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=1, sampler=sampler)\n",
        "\n",
        "test_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in test_ind], targets=targets[test_ind])\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvstu1-sldC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a5cd17-17ce-416b-cbb8-e1108fb454bf"
      },
      "source": [
        "# check out the content of the dataset\n",
        "for i, (x, y) in enumerate(train_loader):\n",
        "    break\n",
        "\n",
        "print(\"x.shape:\", x.shape)\n",
        "print(\"name: \", end=\"\")\n",
        "for letter_onehot in x[0]:\n",
        "    print(all_letters[torch.argmax(letter_onehot)], end=\"\")\n",
        "\n",
        "print(\"\\ny:\", label_to_idx[y.item()])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.shape: torch.Size([1, 10, 52])\n",
            "name: Sniegowski\n",
            "y: polish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3VdtPOhhAQw"
      },
      "source": [
        "## Zadanie 1. (2 pkt.)\n",
        "\n",
        "Zaimplementuj \"zwykłą\" sieć rekurencyjną. \n",
        "![rnn](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "* W klasie `RNN` należy zainicjalizować potrzebne wagi oraz zaimplementować główną logikę dla pojedynczej chwili czasowej $x_t$\n",
        "* Wyjście z sieci możemy mieć dowolny rozmiar, potrzebna jest również warstwa przekształacjąca stan ukryty na wyjście.\n",
        "* W pętli uczenia należy dodać odpowiednie wywołanie sieci. HINT: pamiętać o iterowaniu po wymiarze \"czasowym\".\n",
        "* Zalecane jest użycie aktywacji na warstwie liczącej reprezentacje `hidden` tak, aby nie \"eksplodowała\", np. `tanh`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNu0vccJhAQw"
      },
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 input_size: int,\n",
        "                 hidden_size: int, \n",
        "                 output_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        :param output_size: int\n",
        "            Desired dimensionality of the output vector\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.input_to_hidden = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        \n",
        "        self.hidden_to_output = torch.nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    # for the sake of simplicity a single forward will process only a single timestamp \n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        :param input: torch.tensor \n",
        "            Input tesnor for a single observation at timestep t\n",
        "            shape [batch_size, input_size]\n",
        "        :param hidden: torch.tensor\n",
        "            Representation of the memory of the RNN from previous timestep\n",
        "            shape [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        combined = torch.cat([input, hidden], dim=1) \n",
        "        hidden = torch.nn.Tanh()(self.input_to_hidden(combined))\n",
        "        output = self.hidden_to_output(hidden)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns initial value for the hidden state\n",
        "        \"\"\"\n",
        "        return torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIe3L-8LhAQw"
      },
      "source": [
        "### Pętla uczenia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXEsqqvxhAQx",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108c1f98-89ef-4e5c-e1f8-f9ad9f2e00cd"
      },
      "source": [
        "n_class = len(label_to_idx)\n",
        "\n",
        "# initialize network and optimizer\n",
        "rnn = RNN(n_letters, 256, n_class).cuda()\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)   \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epochs = 1\n",
        "\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    loss_buffer = []\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):  \n",
        "        \n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # get initial hidden state\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "        \n",
        "        # get output for the sample, remember that we treat it as a sequence\n",
        "        # so you need to iterate over the 2nd, time dimensiotn\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        \n",
        "        for t in range(seq_len - 1):\n",
        "            _, hidden = rnn(x[:, t, :], hidden)\n",
        "        output = rnn(x[:, -1, :], hidden)[0]\n",
        "            \n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        \n",
        "        loss_buffer.append(loss.item())\n",
        "        \n",
        "        if i % 1000 == 1:\n",
        "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
        "            loss_buffer = []\n",
        "    \n",
        "\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    ps = []\n",
        "    ys = []\n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "        x = x.cuda()\n",
        "        ys.append(y.numpy())\n",
        "\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        for t in range(seq_len - 1):\n",
        "            _, hidden = rnn(x[:, t, :], hidden)\n",
        "        output = rnn(x[:, -1, :], hidden)[0]\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        ps.append(pred.cpu().numpy())\n",
        "    \n",
        "    ps = np.concatenate(ps, axis=0)\n",
        "    ys = np.concatenate(ys, axis=0)\n",
        "    f1 = f1_score(ys, ps, average='weighted')\n",
        "    \n",
        "    print(f\"Final F1 score: {f1:.2f}\")\n",
        "    assert f1 > 0.15, \"You should get over 0.15 f1 score, try changing some hiperparams!\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Progress:  0% Loss: 2.886\n",
            "Epoch: 0 Progress:  6% Loss: 2.861\n",
            "Epoch: 0 Progress: 11% Loss: 2.766\n",
            "Epoch: 0 Progress: 17% Loss: 2.496\n",
            "Epoch: 0 Progress: 22% Loss: 2.291\n",
            "Epoch: 0 Progress: 28% Loss: 2.148\n",
            "Epoch: 0 Progress: 33% Loss: 1.977\n",
            "Epoch: 0 Progress: 39% Loss: 1.924\n",
            "Epoch: 0 Progress: 44% Loss: 1.894\n",
            "Epoch: 0 Progress: 50% Loss: 1.792\n",
            "Epoch: 0 Progress: 55% Loss: 1.796\n",
            "Epoch: 0 Progress: 61% Loss: 1.774\n",
            "Epoch: 0 Progress: 66% Loss: 1.746\n",
            "Epoch: 0 Progress: 72% Loss: 1.726\n",
            "Epoch: 0 Progress: 77% Loss: 1.656\n",
            "Epoch: 0 Progress: 83% Loss: 1.697\n",
            "Epoch: 0 Progress: 89% Loss: 1.643\n",
            "Epoch: 0 Progress: 94% Loss: 1.673\n",
            "Epoch: 0 Progress: 100% Loss: 1.662\n",
            "Final F1 score: 0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNeNU93qn7BC"
      },
      "source": [
        "## Zadanie 2. (0.5 pkt.)\n",
        "Zaimplementuj funkcje `predict`, która przyjmuje nazwisko w postaci stringa oraz model RNN i wypisuje 3 najlepsze predykcje narodowości dla tego nazwiska razem z ich logitami.\n",
        "\n",
        "**Hint**: Przyda się tutaj jedna z funkcji z pierwszej komórki notebooka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8FhF_08hAQy"
      },
      "source": [
        "def predict(name: str, rnn: RNN):\n",
        "    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n",
        "    tensor = line_to_tensor(name)\n",
        "    tensor = tensor.view(1, *tensor.shape).cuda()\n",
        "    hidden = rnn.init_hidden(1)\n",
        "    for t in range(len(name)):\n",
        "        output, hidden = rnn(tensor[:, t, :], hidden)\n",
        "    logits, indices = torch.sort(output[0], descending=True)\n",
        "    for l, i in zip(logits[:3], indices[:3]):\n",
        "        print(\"\\t\", label_to_idx[int(i)], float(l))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4OWP8wqhAQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51719e2f-981d-464f-cded-5b6a153a69e0"
      },
      "source": [
        "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
        "\n",
        "for name in some_names:\n",
        "    print(name)\n",
        "    predict(name, rnn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Satoshi\n",
            "\t italian 3.7912449836730957\n",
            "\t japanese 2.2028489112854004\n",
            "\t spanish 1.1157729625701904\n",
            "Jackson\n",
            "\t scottish 4.44012451171875\n",
            "\t irish 2.8323991298675537\n",
            "\t english 2.5166213512420654\n",
            "Schmidhuber\n",
            "\t german 3.363201141357422\n",
            "\t dutch 2.515110731124878\n",
            "\t czech 1.7448205947875977\n",
            "Hinton\n",
            "\t scottish 2.6894121170043945\n",
            "\t irish 2.480625867843628\n",
            "\t english 2.0234460830688477\n",
            "Kowalski\n",
            "\t polish 5.826296806335449\n",
            "\t japanese 3.1261284351348877\n",
            "\t russian 3.038498640060425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNETvP06hAQz"
      },
      "source": [
        "## Zadanie 3 (4 pkt.)\n",
        "Ostatnim zadaniem jest implementacji komórki i sieci LSTM. \n",
        "\n",
        "![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "* W klasie `LSTMCell` ma znaleźć się główna loginka LSTMa, czyli wszystkie wagi do stanów `hidden` i `cell` jak i bramek kontrolujących te stany. \n",
        "* W klasie `LSTM` powinno znaleźć się wywołanie komórki LSTM, HINT: poprzednio było w pętli uczenia, teraz przenisiemy to do klasy modelu.\n",
        "* W pętli uczenia należy uzupełnić brakujące wywołania do uczenia i ewaluacji modelu.\n",
        "\n",
        "Zdecydowanie polecam [materiały Chrisa Olaha](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) do zarówno zrozumienia jak i ściągi do wzorów.\n",
        "\n",
        "Zadaniem jest osiągnięcie wartości `f1_score` lepszej niż na sieci RNN, przy prawidłowej implementacji nie powinno być z tym problemów używając podanych hiperparametrów. Dozwolona jest oczywiście zmiana `random seed`.\n",
        "\n",
        "#### Komórka LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNKRxYwChAQz"
      },
      "source": [
        "class LSTMCell(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMCell, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # initialize LSTM weights \n",
        "        # NOTE: there are different approaches that are all correct \n",
        "        # (e.g. single matrix for all input opperations), you can pick\n",
        "        # whichever you like for this task\n",
        "\n",
        "        self.forgetting_layer = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.input_layer = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.cell_layer = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output_layer = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \n",
        "        hidden, cell = states\n",
        "        \n",
        "        # Compute input, forget, and output gates\n",
        "        # then compute new cell state and hidden state\n",
        "        # see http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
        "\n",
        "        combined = torch.cat((hidden, input), dim=1)\n",
        "\n",
        "        forgetting = torch.nn.Sigmoid()(self.forgetting_layer(combined))\n",
        "        input = torch.nn.Tanh()(self.input_layer(combined))\n",
        "        candidates = torch.nn.Sigmoid()(self.cell_layer(combined))\n",
        "        cell = forgetting * cell + input * candidates\n",
        "        \n",
        "        hidden = torch.tanh(cell) * torch.nn.Sigmoid()(self.output_layer(combined))\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U5U8kizhAQz"
      },
      "source": [
        "### Klasa modelu LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2MyIu3_hAQz"
      },
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        \n",
        "    def forward(self, \n",
        "                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        :param input: torch.tensor \n",
        "            Input tesnor for a single observation at timestep t\n",
        "            shape [batch_size, input_size]\n",
        "        Returns Tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size = input.shape[0]\n",
        "        \n",
        "        hidden, cell = self.init_hidden_cell(batch_size)\n",
        "        \n",
        "        hiddens = []\n",
        "        cells = []\n",
        "        \n",
        "        # this time we will process the whole sequence in the forward method\n",
        "        # as oppose to the previous exercise, remember to loop over the timesteps\n",
        "        \n",
        "        time_steps = input.shape[1]\n",
        "\n",
        "        for t in range(time_steps):\n",
        "            hidden, cell = self.cell(input[:, t, :], (hidden, cell))\n",
        "            hiddens.append(hidden)\n",
        "            cells.append(cell)\n",
        "\n",
        "        return hiddens, cells\n",
        "    \n",
        "    def init_hidden_cell(self, batch_size):\n",
        "        \"\"\"\n",
        "        Returns initial value for the hidden and cell states\n",
        "        \"\"\"\n",
        "        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda(), \n",
        "                torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qRxPI-nhAQz"
      },
      "source": [
        "### Pętla uczenia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LVCWqsVhAQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4487878b-7e79-4ae5-d8f5-d4827a4447e0"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "# torch.manual_seed(1337)\n",
        "\n",
        "# build data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "# initialize the lstm with an additional cliassifier layer at the top\n",
        "lstm = LSTM(input_size=len(all_letters), hidden_size=128).cuda()\n",
        "clf = torch.nn.Linear(in_features=128, out_features=len(label_to_idx)).cuda()\n",
        "\n",
        "# initialize a optimizer\n",
        "params = chain(lstm.parameters(), clf.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.01) \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epoch = 1\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epoch):\n",
        "    \n",
        "    loss_buffer = []\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):   \n",
        "        \n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # get output for the sample, remember that we treat it as a sequence\n",
        "        # so you need to iterate over the sequence length here\n",
        "        # don't forget about the classifier!\n",
        "        \n",
        "        output = clf(lstm(x)[1][-1])\n",
        "\n",
        "        # calucate the loss\n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()                                \n",
        "        \n",
        "        loss_buffer.append(loss.item())\n",
        "        \n",
        "        if i % 1000 == 1:\n",
        "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
        "            loss_buffer = []\n",
        "\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    \n",
        "    ps = []\n",
        "    ys = []\n",
        "    for i, (x, y) in enumerate(test_loader): \n",
        "        \n",
        "        x = x.cuda()\n",
        "        ys.append(y.numpy())\n",
        "        \n",
        "        output = clf(lstm(x)[1][-1])\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        ps.append(pred.cpu().numpy())\n",
        "    \n",
        "    ps = np.concatenate(ps, axis=0)\n",
        "    ys = np.concatenate(ys, axis=0)\n",
        "    f1 = f1_score(ys, ps, average='weighted')\n",
        "    \n",
        "    print(f\"Final F1 score: {f1:.2f}\")\n",
        "    assert f1 > 0.18, \"You should get over 0.18 f1 score, try changing some hiperparams!\""
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Progress:  0% Loss: 2.894\n",
            "Epoch: 0 Progress:  6% Loss: 2.381\n",
            "Epoch: 0 Progress: 11% Loss: 1.921\n",
            "Epoch: 0 Progress: 17% Loss: 1.707\n",
            "Epoch: 0 Progress: 22% Loss: 1.565\n",
            "Epoch: 0 Progress: 28% Loss: 1.572\n",
            "Epoch: 0 Progress: 33% Loss: 1.305\n",
            "Epoch: 0 Progress: 39% Loss: 1.176\n",
            "Epoch: 0 Progress: 44% Loss: 1.105\n",
            "Epoch: 0 Progress: 50% Loss: 1.100\n",
            "Epoch: 0 Progress: 55% Loss: 1.039\n",
            "Epoch: 0 Progress: 61% Loss: 1.007\n",
            "Epoch: 0 Progress: 66% Loss: 0.939\n",
            "Epoch: 0 Progress: 72% Loss: 0.912\n",
            "Epoch: 0 Progress: 77% Loss: 0.881\n",
            "Epoch: 0 Progress: 83% Loss: 0.795\n",
            "Epoch: 0 Progress: 89% Loss: 0.901\n",
            "Epoch: 0 Progress: 94% Loss: 0.810\n",
            "Epoch: 0 Progress: 100% Loss: 0.758\n",
            "Final F1 score: 0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGXUhgroo7AN"
      },
      "source": [
        "## Zadanie 4. (0.5 pkt.)\n",
        "Zaimplementuj analogiczną do funkcji `predict` z zadania 2 dla modelu `lstm+clf`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ChJv1fphAQ0"
      },
      "source": [
        "def predict_lstm(name: str, lstm: LSTM, clf: torch.nn.Module):\n",
        "    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n",
        "    tensor = line_to_tensor(name)\n",
        "    tensor = tensor.view(1, *tensor.shape).cuda()\n",
        "    output = clf(lstm(tensor)[1][-1])[0]\n",
        "    logits, indices = torch.sort(output, descending=True)\n",
        "    for l, i in zip(logits[:3], indices[:3]):\n",
        "        print(\"\\t\", label_to_idx[int(i)], float(l))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgQcGWqthAQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620c2b09-a1e9-46f7-f39b-27b7b35ce8ec"
      },
      "source": [
        "# test your lstm predictor\n",
        "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
        "    \n",
        "for name in some_names:\n",
        "    print(name)\n",
        "    predict_lstm(name, lstm, clf)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Satoshi\n",
            "\t japanese 9.24991226196289\n",
            "\t italian 6.910540580749512\n",
            "\t russian 5.170266151428223\n",
            "Jackson\n",
            "\t scottish 15.353461265563965\n",
            "\t english 6.422601699829102\n",
            "\t french 1.0398693084716797\n",
            "Schmidhuber\n",
            "\t german 4.486064434051514\n",
            "\t czech 2.473479747772217\n",
            "\t dutch 2.399200439453125\n",
            "Hinton\n",
            "\t scottish 7.10476541519165\n",
            "\t english 4.8557610511779785\n",
            "\t chinese 2.0448806285858154\n",
            "Kowalski\n",
            "\t polish 8.943662643432617\n",
            "\t czech 3.258988618850708\n",
            "\t japanese 3.168221950531006\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}