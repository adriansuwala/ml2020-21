{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETFpFlCmeuSB"
      },
      "source": [
        "# Modele Seq2Seq i atencja\n",
        "Poniższy notebook jest inspirowany tym tutorialem PyTorcha: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html. Zachęcam żeby zajrzeć tam po więcej informacji.\n",
        "\n",
        "W tym notebooku będziemy próbować rozwiązać problem automatycznego tłumaczenia zdań z jednego języka naturalnego na drugi -- konkretniej z języka polskiego na angielski. Dla przykładu model otrzymujący zdanie:\n",
        "\n",
        "> Myślę, że mnie okłamałeś\n",
        "\n",
        "Powinien zwrócić zdanie\n",
        "> I think you lied to me.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do7k4lf0zsxu"
      },
      "source": [
        "# Importy i przygotowanie danych\n",
        "Poniżej znajdują się importy bibliotek potrzebnych do rozwiązania problemu a także skrypt do ładowania zbioru danych zawierającego pary zdań w języku polskim i angielskim. Poniższy kod można odpalić i schować, ale zachęcamy do zaznajomienia się z tym jak wygląda obróbka danych.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCAgsgj3d7Kf"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1yFSn0ld7Ki"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUVJHRTXKoWh",
        "outputId": "f07b3081-a1f1-4cf5-d95f-1c7fe79418ce"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/gmum/ml2020-21/master/lab/resources/pol.txt"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-27 11:33:30--  https://raw.githubusercontent.com/gmum/ml2020-21/master/lab/resources/pol.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5865362 (5.6M) [text/plain]\n",
            "Saving to: ‘pol.txt.3’\n",
            "\n",
            "pol.txt.3           100%[===================>]   5.59M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-01-27 11:33:30 (77.4 MB/s) - ‘pol.txt.3’ saved [5865362/5865362]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KnEaLlNpire"
      },
      "source": [
        "Poniżej przygotowujemy klasę `Lang` która będzie służyła jako struktura do obsługiwania naszego języka (osobna dla angielskiego i polskiego w naszym przypadku). Do każdego słowa w języku przypisujemy indeks (liczbę porządkową identyfikującą słowo). Dodatkowo definiujemy trzy dodatkowe indeksy:\n",
        "\n",
        "* 0 dla początku zdania (Start of Sentence, SOS)\n",
        "* 1 dla końca zdania (End of Sentence, EOS)\n",
        "* 2 dla paddingu (\"pustych\" wartości). Wartościami tymi będziemy wypełniać zdania w batchu tak, żeby wszystkie były równej długości -- dzięki temu łatwiej będzie zrównoleglić przetwarzanie ich na GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3DtzxXTd7Kk"
      },
      "source": [
        "\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "        self.n_words = 3 # Count SOS, EOS and PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrV27D5Xryh1"
      },
      "source": [
        "Funkcje do normalizowania wchodzących zdań - zamieniamy Unicode na ASCII, zamieniamy wszystkie wielkie litery na małe itd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLLuOE80d7Kk"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = s.replace(\"ł\", \"l\")\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(f'{lang1}.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ld3aJ9Nd7Kl"
      },
      "source": [
        "Wyrzućmy zdania które są zbyt długie (ponad 20 słów)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6fbLbiQd7Km"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) <= MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csmhl8Iod7Km"
      },
      "source": [
        "Pełny proces przetwarzania danych wygląda następująco:\n",
        "\n",
        "- Wczytujemy plik z danymi, dzielimy go na pary zdań.\n",
        "- Normalizujemy tekst\n",
        "- Zamieniamy zdania w listy słów.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRd-fNMAd7Km",
        "outputId": "9f43a85c-e92a-4c89-c761-bff92fa3b9d2"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(pairs[0])\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('pol', 'eng', True)\n",
        "print(\"Przykładowe pary zdań:\")\n",
        "for _ in range(3):\n",
        "    print(random.choice(pairs))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "['idz .', 'go .']\n",
            "Read 40389 sentence pairs\n",
            "Trimmed to 40289 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 20475\n",
            "pol 8084\n",
            "Przykładowe pary zdań:\n",
            "['glod najlepszym kucharzem .', 'hunger is the best sauce .']\n",
            "['ta rozmowa nic nam nie da .', 'that kind of talk will get you nowhere .']\n",
            "['zamierzam tam pojsc .', 'i intend to go there .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQc1t2ArshOa"
      },
      "source": [
        "Na koniec definiujemy jeszcze funkcje, które pozwolą nam zamienić zdania w tensory, które nasza sieć będzie w stanie zrozumieć."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leinIgjGd7Kq"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "def pad_sequences(data_batch):\n",
        "    pl_batch, en_batch = [], []\n",
        "    for pl_sentence, en_sentence in data_batch:\n",
        "        pl_batch += [pl_sentence]\n",
        "        en_batch += [en_sentence]\n",
        "    pl_batch = pad_sequence(pl_batch, padding_value=PAD_token, batch_first=True)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_token, batch_first=True)\n",
        "    return pl_batch, en_batch\n",
        "\n",
        "def prepare_dataset(batch_size):\n",
        "    rng = np.random.RandomState(567)\n",
        "    indices = np.arange(len(pairs))\n",
        "    rng.shuffle(indices)\n",
        "    train_indices = indices[:int(len(pairs) * 0.8)]\n",
        "    test_indices = indices[int(len(pairs) * 0.8):]\n",
        "    train_pairs = list(pairs[idx] for idx in train_indices)\n",
        "    test_pairs = list(pairs[idx] for idx in test_indices)\n",
        "    tensor_train_pairs = [tensorsFromPair(pairs[idx]) for idx in train_indices]\n",
        "    tensor_test_pairs = [tensorsFromPair(pairs[idx]) for idx in test_indices]\n",
        "\n",
        "    train_loader = DataLoader(tensor_train_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    test_loader = DataLoader(tensor_test_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    return train_pairs, test_pairs, train_loader, test_loader"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf844auxd7Km"
      },
      "source": [
        "# Model Seq2Seq\n",
        "\n",
        "\n",
        "\n",
        "W tym celu wykorzystany rekurencyjne sieci neuronowe (RNN-y), które poznaliśmy na poprzednich zajęciach. Konkretniej zbudujemy za ich pomocą model Sequence to Sequence (Seq2Seq), w której wykorzystamy dwie sieci rekurencyjne:\n",
        "1. Enkoder, który będzie przyjmował kolejno słowa ze zdania wejściowego i kompresował informacje o nich w swoim stanie ukrytym.\n",
        "2. Dekoder, który będzie generował kolejne słowa w języku docelowym. \n",
        "\n",
        "![seq2seq](https://docs.chainer.org/en/stable/_images/seq2seq.png)\n",
        "Źródło: https://docs.chainer.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxaO-7_LtPUk"
      },
      "source": [
        "## Funkcje pomocnicze i ewaluacyjne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Zm6_MRd7Kq"
      },
      "source": [
        "def predict(encoder, decoder, inputs, targets=None, max_len=MAX_LENGTH):\n",
        "    batch_size = inputs.size(0)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_output, decoder_attention = decoder(\n",
        "        decoder_input,\n",
        "        decoder_hidden,\n",
        "        targets=targets,\n",
        "        max_len=max_len,\n",
        "        encoder_outputs=encoder_outputs)\n",
        "    return decoder_output, decoder_attention\n",
        "\n",
        "def translate(encoder, decoder, sentence):\n",
        "    inputs = tensorFromSentence(input_lang, sentence).unsqueeze(0).cuda()\n",
        "    decoder_output, decoder_attention = predict(encoder, decoder, inputs)\n",
        "\n",
        "    decoded_words = []\n",
        "    for word in decoder_output[0]:\n",
        "        top_word = word.argmax(-1).item()\n",
        "        decoded_words.append(output_lang.index2word[top_word])\n",
        "        if top_word == EOS_token:\n",
        "            break\n",
        "\n",
        "    if decoder_attention is not None:\n",
        "        # [out_words, in_words]\n",
        "        att = decoder_attention.cpu().detach().numpy()\n",
        "        att = att[0, :len(decoded_words), :]\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        ax.imshow(att)\n",
        "        ax.set_xticklabels([''] + sentence.split(' ') +\n",
        "                        ['EOS'], rotation=90)\n",
        "        ax.set_yticklabels([''] + decoded_words)\n",
        "\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return decoded_words\n",
        "        \n",
        "def translate_randomly(encoder, decoder, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = translate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKEO71h8_mvq"
      },
      "source": [
        "## Pętla trenująca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2hJ24pEd7Kq"
      },
      "source": [
        "def train(encoder, decoder, lr=0.01, batch_size=256, teacher_forcing_ratio=0.5, epochs_num=100):\n",
        "\n",
        "    # Prepare dataset, loss functions, optimizer\n",
        "    train_pairs, test_pairs, train_loader, test_loader = prepare_dataset(batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "\n",
        "    for epoch in range(epochs_num + 1):\n",
        "\n",
        "        # Training\n",
        "        epoch_train_loss = 0.\n",
        "        for in_batch, out_batch in train_loader:\n",
        "            in_batch, out_batch = in_batch.cuda(), out_batch.cuda()\n",
        "\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "        \n",
        "            teacher_inputs = out_batch if random.random() < teacher_forcing_ratio else None\n",
        "        \n",
        "            decoder_output, decoded_attention = predict(\n",
        "                encoder, decoder, in_batch,\n",
        "                targets=teacher_inputs,\n",
        "                max_len=out_batch.size(1)\n",
        "            )\n",
        "        \n",
        "            loss = criterion(decoder_output.transpose(1, 2), out_batch)\n",
        "            loss.backward()\n",
        "        \n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % 25 == 0:\n",
        "            with torch.no_grad():\n",
        "                print(\"=\" * 25, \"Translation test\", \"=\" * 25)\n",
        "                translate_randomly(encoder, decoder, test_pairs, n=5)\n",
        "\n",
        "        mean_train_loss = epoch_train_loss / len(train_loader)\n",
        "        print(f\"Epoch: {epoch}. Train loss: {mean_train_loss}\")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkvoS5hCjgI2"
      },
      "source": [
        "# Zadanie 1 - Dekoder w Seq2Seq (4 pkt.)\n",
        "\n",
        "W tym zadaniu należy zaimplementować dekoder z modelu Seq2Seq. Kod enkodera jest dostępny poniżej i ma Państwu ułatwić odpowiednie zaimplementowanie dekodera.\n",
        "\n",
        "\n",
        "Dekoder otrzymuje na wejściu następujące argumenty:\n",
        "- `input` - tensor o wymiarach `[batch_size, 1]`  zawierający tokeny SOS oznaczające początek zdania (<BOS> na obrazku).\n",
        "- `targets` - `None` albo `torch.tensor` o wymiarach `[batch_size, seq_len]` zawierający indeksy słów w języku docelowym. Jeżeli jest podany to należy zaimplementować teacher forcing na jego podstawie.\n",
        "- `max_len` - Długość sekwencji, którą mamy zwrócić.\n",
        "- `encoder_outputs` - w tym zadaniu ten argument należy zignorować, przyda się dopiero w kolejnym zadaniu.\n",
        "\n",
        "Dekoder ma zwrócić dwie zmienne:\n",
        "- `output` - tensor o wymiarach `[batch_size, seq_len, vocab_size]` reprezentujące logity, które po zaaplikowaniu softmaksa (co będzie zrobione już poza dekoderem) będą reprezentowały prawdopodobieństwa słów przewidzianych przez nasz dekoder.\n",
        "- `attention_weights` - w tym zadaniu należy zawsze zwracać `None`.\n",
        "\n",
        "\n",
        "**HINT 1**: Warto pamiętać o argumencie `batch_first=True` przy definiowaniu RNN-a.\n",
        "\n",
        "**HINT 2**: W enkoderze mogliśmy użyć jednego wywołania klasy LSTM, jako że od razu mieliśmy wszystkie wejścia (słowa języka wejściowego). W przypadku dekodera nie jest to możliwe, jako że wejściem w kroku `t+1` jest wyjście z kroku `t`. Oznacza to że prawdopodobnie potrzebna będzie pętla `for`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6FSVbB0d7Kn"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.lstm(embedded)\n",
        "        return output, hidden"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiBbM1IVd7Ko"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size) \n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
        "        result = None\n",
        "        for i in range(max_len):\n",
        "            embedded = self.embedding(input if targets is None else targets[:, i].unsqueeze(1))\n",
        "            output, hidden = self.lstm(embedded, hidden)\n",
        "            output = self.lin(output)\n",
        "            result = output if result is None else torch.cat((result, output), dim=1)\n",
        "            input = nn.Softmax(dim=2)(output).argmax(dim=2)\n",
        "        return result, None"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0EZJ74SAKFS",
        "outputId": "593e99f0-ea61-4881-e77a-a0759ef79c8e"
      },
      "source": [
        "hidden_size = 128\n",
        "embedding_size = 256\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "decoder = DecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "\n",
        "train(encoder, decoder, lr=0.005, epochs_num=100)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================= Translation test =========================\n",
            "> tomek jest wyzszy od wszystkich innych dzieci w klasie .\n",
            "= tom is taller than any of the other kids in his class .\n",
            "< i i . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "> kioto ma duzo uniwersytetow .\n",
            "= kyoto has many universities .\n",
            "< i i . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "> kto odpowiedzial ?\n",
            "= who responded ?\n",
            "< i i . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "> musisz wziac to pod uwage .\n",
            "= you have to take that into account .\n",
            "< i i . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "> powiedz nam wiecej .\n",
            "= tell us more .\n",
            "< i i . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Epoch: 0. Train loss: 4.323974558285305\n",
            "Epoch: 1. Train loss: 3.0649927490287356\n",
            "Epoch: 2. Train loss: 2.7024922891268655\n",
            "Epoch: 3. Train loss: 2.434321341888299\n",
            "Epoch: 4. Train loss: 2.197949752329834\n",
            "Epoch: 5. Train loss: 2.751747729286315\n",
            "Epoch: 6. Train loss: 2.5202347403540024\n",
            "Epoch: 7. Train loss: 2.105283638225898\n",
            "Epoch: 8. Train loss: 2.084834266217455\n",
            "Epoch: 9. Train loss: 2.0097625513841946\n",
            "Epoch: 10. Train loss: 2.2340202005580068\n",
            "Epoch: 11. Train loss: 1.8523713356430924\n",
            "Epoch: 12. Train loss: 2.3189331165751415\n",
            "Epoch: 13. Train loss: 2.3086291120372833\n",
            "Epoch: 14. Train loss: 1.9588890770849372\n",
            "Epoch: 15. Train loss: 1.6533321124198477\n",
            "Epoch: 16. Train loss: 2.0860127675723996\n",
            "Epoch: 17. Train loss: 1.937770947581157\n",
            "Epoch: 18. Train loss: 1.9030339371279947\n",
            "Epoch: 19. Train loss: 1.8719628073038563\n",
            "Epoch: 20. Train loss: 2.1233380839509506\n",
            "Epoch: 21. Train loss: 1.7000015653921143\n",
            "Epoch: 22. Train loss: 1.6975058749769\n",
            "Epoch: 23. Train loss: 1.591924844841872\n",
            "Epoch: 24. Train loss: 1.7678228711546768\n",
            "========================= Translation test =========================\n",
            "> podrzuce cie do domu .\n",
            "= i ll give you a lift home .\n",
            "< they re t be . . EOS\n",
            "\n",
            "> zagrajmy w gry wideo dla zabicia czasu .\n",
            "= let s play some video games to kill time .\n",
            "< the were were in the in the . . EOS\n",
            "\n",
            "> wybierz miedzy tymi dwoma .\n",
            "= choose between these two .\n",
            "< the of of the the . . EOS\n",
            "\n",
            "> ona jeszcze nie slyszala najnowszych wiadomosci .\n",
            "= she s not yet heard the news .\n",
            "< she is not not at the . . EOS\n",
            "\n",
            "> jego praca polega na robieniu zegarkow .\n",
            "= his work is making watches .\n",
            "< his is is to a . . EOS\n",
            "\n",
            "Epoch: 25. Train loss: 1.9476843394656917\n",
            "Epoch: 26. Train loss: 1.6506804483979645\n",
            "Epoch: 27. Train loss: 1.682785767326427\n",
            "Epoch: 28. Train loss: 1.807830022659064\n",
            "Epoch: 29. Train loss: 1.4005914780658684\n",
            "Epoch: 30. Train loss: 1.3674437416983503\n",
            "Epoch: 31. Train loss: 1.5992242748838006\n",
            "Epoch: 32. Train loss: 1.6682463741384534\n",
            "Epoch: 33. Train loss: 1.402840210947149\n",
            "Epoch: 34. Train loss: 1.5808275980867506\n",
            "Epoch: 35. Train loss: 1.3222866523659802\n",
            "Epoch: 36. Train loss: 1.3026658713462806\n",
            "Epoch: 37. Train loss: 1.304917068568073\n",
            "Epoch: 38. Train loss: 1.6429734778431584\n",
            "Epoch: 39. Train loss: 1.1467072486393302\n",
            "Epoch: 40. Train loss: 1.3703926576981469\n",
            "Epoch: 41. Train loss: 1.3103496033738973\n",
            "Epoch: 42. Train loss: 1.2655403615042036\n",
            "Epoch: 43. Train loss: 1.4252870995163296\n",
            "Epoch: 44. Train loss: 1.322283556342997\n",
            "Epoch: 45. Train loss: 1.2405443508196474\n",
            "Epoch: 46. Train loss: 1.3087617959143476\n",
            "Epoch: 47. Train loss: 1.0658839824443151\n",
            "Epoch: 48. Train loss: 1.1267203583773817\n",
            "Epoch: 49. Train loss: 1.3090138464523036\n",
            "========================= Translation test =========================\n",
            "> ross perot otrzymal okolo osiem milionow glosow .\n",
            "= ross perot received about eight million votes .\n",
            "< winning your way i a . . EOS\n",
            "\n",
            "> w stawie jest duzo ryb .\n",
            "= there are a lot of fish in the pond .\n",
            "< bread is essential in in the . EOS\n",
            "\n",
            "> mam kiepska pamiec do imion ale nigdy nie zapominam twarzy .\n",
            "= i m really bad with names but i never forget a face .\n",
            "< i ve been a a a a a a a a a . EOS\n",
            "\n",
            "> na zwiedzenie kioto rok nie wystarczy .\n",
            "= one year is not enough to visit all the places in kyoto .\n",
            "< he entered the the in in the the . EOS\n",
            "\n",
            "> tom i mary tanczyli do muzyki .\n",
            "= tom and mary were dancing to the music .\n",
            "< tom and mary are very to . . EOS\n",
            "\n",
            "Epoch: 50. Train loss: 1.1001311964909768\n",
            "Epoch: 51. Train loss: 1.367006594801159\n",
            "Epoch: 52. Train loss: 1.1604568374986273\n",
            "Epoch: 53. Train loss: 1.2096949598829572\n",
            "Epoch: 54. Train loss: 1.367806555667249\n",
            "Epoch: 55. Train loss: 1.2673381555027194\n",
            "Epoch: 56. Train loss: 1.1855862694570707\n",
            "Epoch: 57. Train loss: 1.2836108517034777\n",
            "Epoch: 58. Train loss: 1.088292326542571\n",
            "Epoch: 59. Train loss: 1.1097429342575311\n",
            "Epoch: 60. Train loss: 1.0348177511838177\n",
            "Epoch: 61. Train loss: 1.0611770952375428\n",
            "Epoch: 62. Train loss: 1.2451969157046978\n",
            "Epoch: 63. Train loss: 1.2480441435371408\n",
            "Epoch: 64. Train loss: 1.194821812254968\n",
            "Epoch: 65. Train loss: 1.0448702903656615\n",
            "Epoch: 66. Train loss: 1.0643770524405962\n",
            "Epoch: 67. Train loss: 0.868377657014034\n",
            "Epoch: 68. Train loss: 1.1032313797844662\n",
            "Epoch: 69. Train loss: 1.1457694679348995\n",
            "Epoch: 70. Train loss: 1.1194222642825769\n",
            "Epoch: 71. Train loss: 1.155257058910109\n",
            "Epoch: 72. Train loss: 1.0379828925213097\n",
            "Epoch: 73. Train loss: 1.0502490872047872\n",
            "Epoch: 74. Train loss: 1.0959580049661564\n",
            "========================= Translation test =========================\n",
            "> przestalo padac .\n",
            "= it has stopped raining .\n",
            "< it s bitter japan . EOS\n",
            "\n",
            "> jesli cos jej sie stanie wlasnorecznie wymierze ci sprawiedliwosc .\n",
            "= if any harm comes to her i will hold you personally responsible .\n",
            "< if i had help you i i i i help . . EOS\n",
            "\n",
            "> nie mogla powstrzymac sie od placzu .\n",
            "= she couldn t help from crying .\n",
            "< don t t have me me . EOS\n",
            "\n",
            "> wlasnie zaczalem grac w tenisa .\n",
            "= i ve just started playing tennis .\n",
            "< i just french french french . EOS\n",
            "\n",
            "> myslalam ze ktos umarl .\n",
            "= i thought someone died .\n",
            "< i thought he was died . EOS\n",
            "\n",
            "Epoch: 75. Train loss: 1.0636569272102196\n",
            "Epoch: 76. Train loss: 1.0090031304585172\n",
            "Epoch: 77. Train loss: 0.9259690956585849\n",
            "Epoch: 78. Train loss: 1.0266656781951453\n",
            "Epoch: 79. Train loss: 1.200202977152667\n",
            "Epoch: 80. Train loss: 0.8526191472146099\n",
            "Epoch: 81. Train loss: 0.969350142096768\n",
            "Epoch: 82. Train loss: 1.0159758260660243\n",
            "Epoch: 83. Train loss: 1.0805756682816463\n",
            "Epoch: 84. Train loss: 0.8600370510239228\n",
            "Epoch: 85. Train loss: 0.8757050078988652\n",
            "Epoch: 86. Train loss: 0.9358108946360604\n",
            "Epoch: 87. Train loss: 0.9744490588893227\n",
            "Epoch: 88. Train loss: 1.0025027140269116\n",
            "Epoch: 89. Train loss: 0.772076114433788\n",
            "Epoch: 90. Train loss: 0.9689025078199819\n",
            "Epoch: 91. Train loss: 1.0184487805448634\n",
            "Epoch: 92. Train loss: 0.9601706013181395\n",
            "Epoch: 93. Train loss: 0.8822093878942912\n",
            "Epoch: 94. Train loss: 0.8808249278541004\n",
            "Epoch: 95. Train loss: 0.8523629312431218\n",
            "Epoch: 96. Train loss: 0.963624234547809\n",
            "Epoch: 97. Train loss: 0.8539429611234467\n",
            "Epoch: 98. Train loss: 0.8141050332416189\n",
            "Epoch: 99. Train loss: 0.9408736505863508\n",
            "========================= Translation test =========================\n",
            "> dalem siostrze lalke .\n",
            "= i gave my sister a doll .\n",
            "< i took advantage of the cake . EOS\n",
            "\n",
            "> poprosze piwo .\n",
            "= a beer please .\n",
            "< he made his . EOS\n",
            "\n",
            "> myslisz ze jestem zdrowy ?\n",
            "= do you think i m healthy ?\n",
            "< am m interested in that bad ? EOS\n",
            "\n",
            "> tom dal wiele wskazowek ale zadna z nich mi sie nie podobala .\n",
            "= tom gave a lot of suggestions but i didn t like any of them .\n",
            "< tom gave a crowd of the of a edge edge of . . EOS\n",
            "\n",
            "> jak daleko jest stad do tokio ?\n",
            "= how far is it from here to tokyo ?\n",
            "< how far is it from the school here ? EOS\n",
            "\n",
            "Epoch: 100. Train loss: 0.8795454802808337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crrwsvmgGHix"
      },
      "source": [
        "# Zadanie 2 - Atencja w RNN-ach (3 pkt.)\n",
        "\n",
        "![seq2seq z dekoderem](https://www.researchgate.net/profile/Chandan_Reddy6/publication/329464533/figure/fig3/AS:701043021197314@1544153089772/An-attention-based-seq2seq-model.ppm)\n",
        "\n",
        "Źródło: https://github.com/google/seq2seq\n",
        "\n",
        "W tym zadaniu należy napisać kod nowego dekodera, który ma działać podobnie jak dekoder w poprzednim zadaniu, ale jednocześnie ma wykorzystywać mechanizm atencji.\n",
        "\n",
        "W normalnym dekoderze, w kroku `t` wejściem do komórki LSTM-a (pomijamy tutaj przekazywanie stanu ukrytego) była wyłącznie zembeddowana reprezentacja $\\bar{y}_t$. W dekoderze z atencją na wejściu podawna będzie konkatenacja tego wektora oraz specjalnego wektora $z_t$ stworzonego na podstawie wyjść z enkodera: $\\tilde{h}_t = [\\bar{y}_t, z_t]$. \n",
        "\n",
        "Wektor $z_t$ jest pozyskiwany za pomocą mechanizmu atencji. Intuicyjnie chcielibyśmy w nim zebrać informacje z enkodera, które będą najistotniejsze przy dekodowaniu aktualnego słowa. Przyjmijmy, że mamy funkcję alignmentu $a(h, e)$, która jest nam w stanie powiedzieć jak bardzo podobne do siebie są stan ukryty dekodera $h$ oraz reprezentacja enkodera dla słowa $e$.\n",
        "\n",
        "Wtedy \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "w_i &= \\frac{ \\exp(a(h, e_i)) }{\\sum_{j} \\exp(a(h, e_j))} \\\\\n",
        "z_t &= \\sum_i e_i \\cdot w_i\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "W naszym wypadku funkcja alignmentu $a(h, e)$ ma być siecią neuronową z dwoma warstwami o kolejnych wymiarach: `[2 * hidden_size, hidden_size, 1]` i aktywacją tanh po pierwszej warstwie.\n",
        "\n",
        "Argumenty wejściowe i wyjściowe z dekodera są takie same jak w poprzednim z zadaniu z wyjątkiem:\n",
        "- Tym razem na wejściu otrzymujemy tensor `encoder_outputs` o wymiarach `[batch_size, encoder_seq_len, hidden_size]`. To są reprezentacje $e_i$, które należy wykorzystać w mechanizmie atencji.\n",
        "- Tym razem na wyjściu `attention_weights` powinno być tensorem o wymiarach `[batch_size, decoder_seq_len, encoder_seq_len]` zawierającym wagi $w_i$. **HINT:** wartości tego tensora powinny się sumować do jedynki na ostatnim wymiarze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bETuU9Hd7Kp"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, vocab_size)\n",
        "        self.alignment = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
        "        result = None\n",
        "        seq_att_weights = None\n",
        "        for i in range(max_len):\n",
        "            stacked = torch.stack([hidden[0].squeeze(0)] * encoder_outputs.shape[1], dim=1)\n",
        "            att_scores = self.alignment(torch.cat((stacked, encoder_outputs), dim=2))\n",
        "            att_weights = nn.Softmax(dim=1)(att_scores)\n",
        "            att_output = torch.sum(att_weights * encoder_outputs, dim=1)\n",
        "            \n",
        "            embedded = self.embedding(input if targets is None else targets[:, i].unsqueeze(1))\n",
        "            output, hidden = self.lstm(torch.cat((embedded, att_output.unsqueeze(1)), dim=2), hidden)\n",
        "            output = self.lin(output)\n",
        "            result = output if result is None else torch.cat((result, output), dim=1)\n",
        "            input = nn.Softmax(dim=2)(output).argmax(dim=2)\n",
        "        return result, seq_att_weights"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mipeLXX9fx_7",
        "outputId": "8e0ee050-ff54-493d-9474-0dfceb5e4ca9"
      },
      "source": [
        "hidden_size = 128\n",
        "embedding_size = 256\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "\n",
        "train(encoder, decoder, lr=0.005, epochs_num=100)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================= Translation test =========================\n",
            "> tom polozyl pokrywe z powrotem na smietniku .\n",
            "= tom put the lid back on the trash can .\n",
            "< tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom\n",
            "\n",
            "> spodziewam sie otrzymac dzis emaila od firmy .\n",
            "= i expect to receive an email from the company today .\n",
            "< tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom\n",
            "\n",
            "> uswiadomilem sobie ze powinienem uczyc sie francuskiego nieco intensywniej .\n",
            "= i realized i needed to study french a little harder .\n",
            "< i is . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "> banany sa zolte .\n",
            "= bananas are yellow .\n",
            "< tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom tom\n",
            "\n",
            "> studenci sa na wakacjach .\n",
            "= the students are on vacation .\n",
            "< i the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "Epoch: 0. Train loss: 4.259778755051749\n",
            "Epoch: 1. Train loss: 3.423019192994587\n",
            "Epoch: 2. Train loss: 2.7167509482020424\n",
            "Epoch: 3. Train loss: 2.1449293273072394\n",
            "Epoch: 4. Train loss: 2.324090012245708\n",
            "Epoch: 5. Train loss: 1.9813087974039336\n",
            "Epoch: 6. Train loss: 1.7930737652239346\n",
            "Epoch: 7. Train loss: 1.762742639415794\n",
            "Epoch: 8. Train loss: 1.9007853318951906\n",
            "Epoch: 9. Train loss: 1.8659456234780096\n",
            "Epoch: 10. Train loss: 1.74024242144965\n",
            "Epoch: 11. Train loss: 1.5478141061473816\n",
            "Epoch: 12. Train loss: 1.5769769543013166\n",
            "Epoch: 13. Train loss: 1.5622592419533738\n",
            "Epoch: 14. Train loss: 1.525405656856795\n",
            "Epoch: 15. Train loss: 1.5387228244605164\n",
            "Epoch: 16. Train loss: 1.4167679287641057\n",
            "Epoch: 17. Train loss: 1.2127914823414314\n",
            "Epoch: 18. Train loss: 1.1265720515290187\n",
            "Epoch: 19. Train loss: 1.315545780343994\n",
            "Epoch: 20. Train loss: 1.0677818858345587\n",
            "Epoch: 21. Train loss: 1.1603698927450865\n",
            "Epoch: 22. Train loss: 1.0043165247835633\n",
            "Epoch: 23. Train loss: 1.1299647109560846\n",
            "Epoch: 24. Train loss: 1.0302772734328987\n",
            "========================= Translation test =========================\n",
            "> myslalem ze wiem co robic .\n",
            "= i thought i knew what to do .\n",
            "< i i know what i do . . EOS\n",
            "\n",
            "> umylismy rece .\n",
            "= we washed our hands .\n",
            "< my hands . EOS\n",
            "\n",
            "> rozumiem ze zatrudniacie tylko ludzi ktorzy znaja wiecej niz jeden jezyk .\n",
            "= i assume you only hire people who speak more than one language .\n",
            "< i assume it the only people who i one more native speakers speakers . . . . EOS\n",
            "\n",
            "> wlasnie przebieglem trzy mile .\n",
            "= i just ran three miles .\n",
            "< i was just a bad . . EOS\n",
            "\n",
            "> struktura rymow tego wiersza jest niezwykle skomplikowana .\n",
            "= the poem s rhyme scheme is highly complex .\n",
            "< i say you this a as beautiful beautiful . . EOS\n",
            "\n",
            "Epoch: 25. Train loss: 0.9323777905037065\n",
            "Epoch: 26. Train loss: 1.0388131531413705\n",
            "Epoch: 27. Train loss: 0.9184269086302569\n",
            "Epoch: 28. Train loss: 0.9080118635625002\n",
            "Epoch: 29. Train loss: 0.8233956892878586\n",
            "Epoch: 30. Train loss: 0.8585779529456641\n",
            "Epoch: 31. Train loss: 0.8464331643707637\n",
            "Epoch: 32. Train loss: 0.7994133676901194\n",
            "Epoch: 33. Train loss: 0.7639843072753311\n",
            "Epoch: 34. Train loss: 0.7423522344657353\n",
            "Epoch: 35. Train loss: 0.7840675814114215\n",
            "Epoch: 36. Train loss: 0.8406940019922331\n",
            "Epoch: 37. Train loss: 0.8437006375050202\n",
            "Epoch: 38. Train loss: 0.7614178229345098\n",
            "Epoch: 39. Train loss: 0.6531713898145844\n",
            "Epoch: 40. Train loss: 0.7385325547610779\n",
            "Epoch: 41. Train loss: 0.6399351398641657\n",
            "Epoch: 42. Train loss: 0.6837422029736141\n",
            "Epoch: 43. Train loss: 0.626437974870131\n",
            "Epoch: 44. Train loss: 0.7346825485139907\n",
            "Epoch: 45. Train loss: 0.5565752425778007\n",
            "Epoch: 46. Train loss: 0.5544080624988835\n",
            "Epoch: 47. Train loss: 0.6812016026519742\n",
            "Epoch: 48. Train loss: 0.5370302003242874\n",
            "Epoch: 49. Train loss: 0.5849097681126468\n",
            "========================= Translation test =========================\n",
            "> nie chce wygladac jakbym nic nie robil .\n",
            "= i don t want to look like i m not doing anything .\n",
            "< i don t want want t want anything . . EOS\n",
            "\n",
            "> muzeum prado jest zamkniete poniewaz dzis jest poniedzialek .\n",
            "= the prado museum is closed because today is monday .\n",
            "< the museum is is because he is it today today . EOS\n",
            "\n",
            "> tom jest wylaczony z powodu grypy .\n",
            "= tom is off with the flu .\n",
            "< tom is a free . . EOS\n",
            "\n",
            "> niedobrze mi . chce mi sie wymiotowac .\n",
            "= i feel very sick . i want to throw up .\n",
            "< i m late late late . . EOS\n",
            "\n",
            "> gdzie jest reszta ludzi ?\n",
            "= where s everyone else ?\n",
            "< the is the people ? EOS\n",
            "\n",
            "Epoch: 50. Train loss: 0.6205224445272636\n",
            "Epoch: 51. Train loss: 0.49090012844784986\n",
            "Epoch: 52. Train loss: 0.6342848285613212\n",
            "Epoch: 53. Train loss: 0.6390170732897426\n",
            "Epoch: 54. Train loss: 0.53186800377813\n",
            "Epoch: 55. Train loss: 0.5358019025316314\n",
            "Epoch: 56. Train loss: 0.5118118102859986\n",
            "Epoch: 57. Train loss: 0.5125855152312268\n",
            "Epoch: 58. Train loss: 0.49211849520326634\n",
            "Epoch: 59. Train loss: 0.5239790418658347\n",
            "Epoch: 60. Train loss: 0.541012788121219\n",
            "Epoch: 61. Train loss: 0.394408048382805\n",
            "Epoch: 62. Train loss: 0.5113286126676636\n",
            "Epoch: 63. Train loss: 0.5279901019705918\n",
            "Epoch: 64. Train loss: 0.408647967204045\n",
            "Epoch: 65. Train loss: 0.4525221479384022\n",
            "Epoch: 66. Train loss: 0.5486032750174802\n",
            "Epoch: 67. Train loss: 0.4801608314414302\n",
            "Epoch: 68. Train loss: 0.5293065829633085\n",
            "Epoch: 69. Train loss: 0.4929789995168528\n",
            "Epoch: 70. Train loss: 0.4933258721592008\n",
            "Epoch: 71. Train loss: 0.4464834725238486\n",
            "Epoch: 72. Train loss: 0.4380735336647679\n",
            "Epoch: 73. Train loss: 0.46917741016359144\n",
            "Epoch: 74. Train loss: 0.4047813490005432\n",
            "========================= Translation test =========================\n",
            "> skladam wypowiedzenie .\n",
            "= i resign .\n",
            "< i m short . . EOS\n",
            "\n",
            "> wiem o co chodzi .\n",
            "= i know what s up .\n",
            "< i know what s going . EOS\n",
            "\n",
            "> to rozsadna cena .\n",
            "= that s a reasonable price .\n",
            "< there s a price . EOS\n",
            "\n",
            "> tom mowi ze musze sie uczyc francuskiego .\n",
            "= tom says i have to study french .\n",
            "< tom says you should to learn french . EOS\n",
            "\n",
            "> w razie potrzeby zawsze mozesz na niego liczyc .\n",
            "= you can always count on him in any emergency .\n",
            "< let s see you can always count to . . EOS\n",
            "\n",
            "Epoch: 75. Train loss: 0.45628133224275347\n",
            "Epoch: 76. Train loss: 0.4769957791496482\n",
            "Epoch: 77. Train loss: 0.43936199330381076\n",
            "Epoch: 78. Train loss: 0.4618766387834007\n",
            "Epoch: 79. Train loss: 0.46120008820527425\n",
            "Epoch: 80. Train loss: 0.4310080759321517\n",
            "Epoch: 81. Train loss: 0.38524357452323393\n",
            "Epoch: 82. Train loss: 0.3629876326627305\n",
            "Epoch: 83. Train loss: 0.39687068904528305\n",
            "Epoch: 84. Train loss: 0.42973307006029665\n",
            "Epoch: 85. Train loss: 0.44940845025629395\n",
            "Epoch: 86. Train loss: 0.4395311355151029\n",
            "Epoch: 87. Train loss: 0.471409639509003\n",
            "Epoch: 88. Train loss: 0.45242394862275215\n",
            "Epoch: 89. Train loss: 0.3894685883045433\n",
            "Epoch: 90. Train loss: 0.33655032302097726\n",
            "Epoch: 91. Train loss: 0.4328705831338459\n",
            "Epoch: 92. Train loss: 0.4018065717001076\n",
            "Epoch: 93. Train loss: 0.43358541891858393\n",
            "Epoch: 94. Train loss: 0.41041465796666987\n",
            "Epoch: 95. Train loss: 0.38007935479335075\n",
            "Epoch: 96. Train loss: 0.43055526282738094\n",
            "Epoch: 97. Train loss: 0.3610978290985619\n",
            "Epoch: 98. Train loss: 0.35911950247735525\n",
            "Epoch: 99. Train loss: 0.31751958586900153\n",
            "========================= Translation test =========================\n",
            "> przychodzilem tutaj z moimi przyjaciolmi .\n",
            "= i used to come here with my friends .\n",
            "< you you ve been my my friends my . EOS\n",
            "\n",
            "> bez pracy nie ma kolaczy .\n",
            "= no pain no gain .\n",
            "< your cars doesn t no . . EOS\n",
            "\n",
            "> dlaczego chcesz uczyc sie francuskiego ?\n",
            "= why do you want to study french ?\n",
            "< i want want to learn french ? EOS\n",
            "\n",
            "> moglbym napisac ksiazke .\n",
            "= i could write a book .\n",
            "< i could write a book . EOS\n",
            "\n",
            "> odrzucila nasza propozycje .\n",
            "= she turned down our proposal .\n",
            "< our team sent person . EOS\n",
            "\n",
            "Epoch: 100. Train loss: 0.33631760570810115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPmEeponfgob"
      },
      "source": [
        "# Powiązana literatura\n",
        "\n",
        "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "## Przydatne tutoriale\n",
        "\n",
        "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "* https://github.com/bentrevett/pytorch-seq2seq\n",
        "* https://github.com/gmum/AppliedDL2020/tree/master/Week%207 - materiały z kursu Applied Deep Learning prowadzonego w semestrze letnim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAoJGLVhV_-y"
      },
      "source": [
        "# Zadanie dodatkowe: Transformer (7 pkt.)\n",
        "\n",
        "Na podstawie pracy [Attention is All You Need](https://arxiv.org/abs/1706.03762) oraz strony [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention) zaimplementuj transformera działającego na powyższym zadaniu tłumaczenia z polskiego na angielski.\n",
        "\n",
        "![transformer](http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)"
      ]
    }
  ]
}